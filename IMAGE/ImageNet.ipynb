{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDvCjTOtjp0z",
        "outputId": "1de61524-980e-49c9-9470-42ad0a779410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=d8f86681499b5f47e385d32b5676ac80ac4529190948ed254f46991fe83cb531\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Conv2D , MaxPooling2D ,BatchNormalization ,Flatten ,Dropout ,\\\n",
        "Activation\n",
        "!pip install tflearn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koF4WF_WTh7q",
        "outputId": "c18add5e-36d1-4596-a2a5-1dbd773621d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100.0% 60276736 / 60270631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully downloaded 17flowers.tgz 60270631 bytes.\n",
            "File Extracted\n",
            "Starting to parse images...\n",
            "Parsing Done!\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, \\\n",
        " Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "import numpy as np\n",
        "import json\n",
        "np.random.seed(1000)\n",
        "\n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk-DkUJugDLw",
        "outputId": "3a593a8d-14d0-4ea4-801b-dc5150c9df85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 22, 22, 256)       614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 22, 22, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 10, 10, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 10, 10, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 384)         885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8, 8, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              4198400   \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 1000)              0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 1000)             4000      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 17)                17017     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 17)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,145,281\n",
            "Trainable params: 28,124,401\n",
            "Non-trainable params: 20,880\n",
            "_________________________________________________________________\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/20\n",
            "1088/1088 [==============================] - ETA: 0s - loss: 2.8325 - acc: 0.2270 - mean_absolute_error: 0.0957 - mean_squared_error: 0.0571"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1088/1088 [==============================] - 100s 92ms/sample - loss: 2.8325 - acc: 0.2270 - mean_absolute_error: 0.0957 - mean_squared_error: 0.0571 - val_loss: 62.4488 - val_acc: 0.0662 - val_mean_absolute_error: 0.1099 - val_mean_squared_error: 0.1095\n",
            "Epoch 2/20\n",
            "1088/1088 [==============================] - 89s 82ms/sample - loss: 2.2090 - acc: 0.3520 - mean_absolute_error: 0.0833 - mean_squared_error: 0.0503 - val_loss: 73.4697 - val_acc: 0.0662 - val_mean_absolute_error: 0.1099 - val_mean_squared_error: 0.1097\n",
            "Epoch 3/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 1.7677 - acc: 0.4651 - mean_absolute_error: 0.0720 - mean_squared_error: 0.0423 - val_loss: 20.9148 - val_acc: 0.0919 - val_mean_absolute_error: 0.1065 - val_mean_squared_error: 0.1019\n",
            "Epoch 4/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 1.5786 - acc: 0.4890 - mean_absolute_error: 0.0685 - mean_squared_error: 0.0402 - val_loss: 15.3633 - val_acc: 0.1765 - val_mean_absolute_error: 0.0971 - val_mean_squared_error: 0.0880\n",
            "Epoch 5/20\n",
            "1088/1088 [==============================] - 90s 82ms/sample - loss: 1.4325 - acc: 0.5533 - mean_absolute_error: 0.0620 - mean_squared_error: 0.0362 - val_loss: 6.7590 - val_acc: 0.2647 - val_mean_absolute_error: 0.0868 - val_mean_squared_error: 0.0718\n",
            "Epoch 6/20\n",
            "1088/1088 [==============================] - 88s 80ms/sample - loss: 1.3295 - acc: 0.5607 - mean_absolute_error: 0.0601 - mean_squared_error: 0.0346 - val_loss: 9.2026 - val_acc: 0.1765 - val_mean_absolute_error: 0.0982 - val_mean_squared_error: 0.0805\n",
            "Epoch 7/20\n",
            "1088/1088 [==============================] - 88s 81ms/sample - loss: 1.1907 - acc: 0.6278 - mean_absolute_error: 0.0541 - mean_squared_error: 0.0307 - val_loss: 4.2674 - val_acc: 0.3382 - val_mean_absolute_error: 0.0785 - val_mean_squared_error: 0.0640\n",
            "Epoch 8/20\n",
            "1088/1088 [==============================] - 88s 81ms/sample - loss: 1.0729 - acc: 0.6471 - mean_absolute_error: 0.0513 - mean_squared_error: 0.0286 - val_loss: 5.8293 - val_acc: 0.2279 - val_mean_absolute_error: 0.0900 - val_mean_squared_error: 0.0788\n",
            "Epoch 9/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 0.9120 - acc: 0.6847 - mean_absolute_error: 0.0450 - mean_squared_error: 0.0252 - val_loss: 3.4552 - val_acc: 0.4044 - val_mean_absolute_error: 0.0716 - val_mean_squared_error: 0.0559\n",
            "Epoch 10/20\n",
            "1088/1088 [==============================] - 88s 81ms/sample - loss: 0.9436 - acc: 0.7031 - mean_absolute_error: 0.0455 - mean_squared_error: 0.0255 - val_loss: 2.4844 - val_acc: 0.4632 - val_mean_absolute_error: 0.0655 - val_mean_squared_error: 0.0486\n",
            "Epoch 11/20\n",
            "1088/1088 [==============================] - 88s 81ms/sample - loss: 0.8541 - acc: 0.7206 - mean_absolute_error: 0.0417 - mean_squared_error: 0.0236 - val_loss: 4.0527 - val_acc: 0.3750 - val_mean_absolute_error: 0.0766 - val_mean_squared_error: 0.0594\n",
            "Epoch 12/20\n",
            "1088/1088 [==============================] - 90s 83ms/sample - loss: 0.7219 - acc: 0.7491 - mean_absolute_error: 0.0365 - mean_squared_error: 0.0204 - val_loss: 3.0544 - val_acc: 0.4191 - val_mean_absolute_error: 0.0693 - val_mean_squared_error: 0.0542\n",
            "Epoch 13/20\n",
            "1088/1088 [==============================] - 88s 81ms/sample - loss: 0.6637 - acc: 0.7730 - mean_absolute_error: 0.0348 - mean_squared_error: 0.0192 - val_loss: 3.9224 - val_acc: 0.3382 - val_mean_absolute_error: 0.0776 - val_mean_squared_error: 0.0620\n",
            "Epoch 14/20\n",
            "1088/1088 [==============================] - 86s 79ms/sample - loss: 0.5737 - acc: 0.8061 - mean_absolute_error: 0.0309 - mean_squared_error: 0.0163 - val_loss: 2.3417 - val_acc: 0.5257 - val_mean_absolute_error: 0.0612 - val_mean_squared_error: 0.0442\n",
            "Epoch 15/20\n",
            "1088/1088 [==============================] - 90s 83ms/sample - loss: 0.5942 - acc: 0.7969 - mean_absolute_error: 0.0310 - mean_squared_error: 0.0173 - val_loss: 2.0056 - val_acc: 0.5331 - val_mean_absolute_error: 0.0588 - val_mean_squared_error: 0.0415\n",
            "Epoch 16/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 0.5144 - acc: 0.8318 - mean_absolute_error: 0.0276 - mean_squared_error: 0.0147 - val_loss: 2.4106 - val_acc: 0.5000 - val_mean_absolute_error: 0.0606 - val_mean_squared_error: 0.0452\n",
            "Epoch 17/20\n",
            "1088/1088 [==============================] - 89s 82ms/sample - loss: 0.3794 - acc: 0.8612 - mean_absolute_error: 0.0230 - mean_squared_error: 0.0118 - val_loss: 2.1577 - val_acc: 0.5074 - val_mean_absolute_error: 0.0583 - val_mean_squared_error: 0.0438\n",
            "Epoch 18/20\n",
            "1088/1088 [==============================] - 89s 82ms/sample - loss: 0.3944 - acc: 0.8575 - mean_absolute_error: 0.0219 - mean_squared_error: 0.0117 - val_loss: 2.3206 - val_acc: 0.5441 - val_mean_absolute_error: 0.0558 - val_mean_squared_error: 0.0419\n",
            "Epoch 19/20\n",
            "1088/1088 [==============================] - 90s 83ms/sample - loss: 0.3834 - acc: 0.8787 - mean_absolute_error: 0.0202 - mean_squared_error: 0.0106 - val_loss: 2.9899 - val_acc: 0.5000 - val_mean_absolute_error: 0.0607 - val_mean_squared_error: 0.0489\n",
            "Epoch 20/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 0.4564 - acc: 0.8336 - mean_absolute_error: 0.0235 - mean_squared_error: 0.0134 - val_loss: 3.4833 - val_acc: 0.4669 - val_mean_absolute_error: 0.0633 - val_mean_squared_error: 0.0515\n"
          ]
        }
      ],
      "source": [
        "#DropOut Model\n",
        "\n",
        "model=Sequential()\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96,input_shape=(224,224,3),kernel_size=(11,11),strides=(4,4)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#1st maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#2nd convolutional layer\n",
        "model.add(Conv2D(filters=256,kernel_size=(5,5),padding='valid',strides=(1,1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#2nd maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#3rd convolutional layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#3rd maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.5))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.5))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
        " metrics=['accuracy','mae','mse'])\n",
        "\n",
        "# (5) Train\n",
        "score = model.fit(x, y, batch_size=64, epochs=20, verbose=1,\\\n",
        "validation_split=0.2, shuffle=True)\n",
        "# opt = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "json.dump(score.history, open(\"DropoutModel\", 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO_2neQXktTz",
        "outputId": "3bfa4287-cbf9-416a-e4ab-58184ada0892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 26, 26, 96)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 26, 26, 96)       384       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 22, 22, 256)       614656    \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 22, 22, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 10, 10, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 10, 10, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 384)         885120    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 8, 8, 384)        1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 2, 2, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              4198400   \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 4096)             16384     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 4096)             16384     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 1000)              0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 1000)             4000      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 17)                17017     \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 17)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,145,281\n",
            "Trainable params: 28,124,401\n",
            "Non-trainable params: 20,880\n",
            "_________________________________________________________________\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/20\n",
            "1088/1088 [==============================] - 87s 80ms/sample - loss: 3.5970 - acc: 0.2776 - mean_absolute_error: 0.0886 - mean_squared_error: 0.0599 - val_loss: 243.8317 - val_acc: 0.0404 - val_mean_absolute_error: 0.1129 - val_mean_squared_error: 0.1129\n",
            "Epoch 2/20\n",
            "1088/1088 [==============================] - 84s 77ms/sample - loss: 1.6037 - acc: 0.4688 - mean_absolute_error: 0.0746 - mean_squared_error: 0.0398 - val_loss: 94.1647 - val_acc: 0.0515 - val_mean_absolute_error: 0.1117 - val_mean_squared_error: 0.1099\n",
            "Epoch 3/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 1.3341 - acc: 0.5496 - mean_absolute_error: 0.0650 - mean_squared_error: 0.0347 - val_loss: 57.8548 - val_acc: 0.0551 - val_mean_absolute_error: 0.1112 - val_mean_squared_error: 0.1111\n",
            "Epoch 4/20\n",
            "1088/1088 [==============================] - 84s 77ms/sample - loss: 1.0925 - acc: 0.6204 - mean_absolute_error: 0.0562 - mean_squared_error: 0.0299 - val_loss: 11.9334 - val_acc: 0.1507 - val_mean_absolute_error: 0.0996 - val_mean_squared_error: 0.0888\n",
            "Epoch 5/20\n",
            "1088/1088 [==============================] - 83s 77ms/sample - loss: 0.9056 - acc: 0.6866 - mean_absolute_error: 0.0492 - mean_squared_error: 0.0255 - val_loss: 5.5518 - val_acc: 0.1618 - val_mean_absolute_error: 0.0985 - val_mean_squared_error: 0.0774\n",
            "Epoch 6/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.7920 - acc: 0.7160 - mean_absolute_error: 0.0430 - mean_squared_error: 0.0225 - val_loss: 2.1219 - val_acc: 0.3750 - val_mean_absolute_error: 0.0798 - val_mean_squared_error: 0.0487\n",
            "Epoch 7/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.6906 - acc: 0.7518 - mean_absolute_error: 0.0378 - mean_squared_error: 0.0201 - val_loss: 4.6884 - val_acc: 0.2537 - val_mean_absolute_error: 0.0886 - val_mean_squared_error: 0.0680\n",
            "Epoch 8/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.6798 - acc: 0.7684 - mean_absolute_error: 0.0350 - mean_squared_error: 0.0191 - val_loss: 4.9809 - val_acc: 0.2610 - val_mean_absolute_error: 0.0883 - val_mean_squared_error: 0.0727\n",
            "Epoch 9/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.6549 - acc: 0.7776 - mean_absolute_error: 0.0347 - mean_squared_error: 0.0191 - val_loss: 3.5979 - val_acc: 0.3088 - val_mean_absolute_error: 0.0834 - val_mean_squared_error: 0.0605\n",
            "Epoch 10/20\n",
            "1088/1088 [==============================] - 83s 77ms/sample - loss: 0.5919 - acc: 0.8024 - mean_absolute_error: 0.0313 - mean_squared_error: 0.0170 - val_loss: 3.9540 - val_acc: 0.3713 - val_mean_absolute_error: 0.0768 - val_mean_squared_error: 0.0586\n",
            "Epoch 11/20\n",
            "1088/1088 [==============================] - 84s 78ms/sample - loss: 0.5349 - acc: 0.8189 - mean_absolute_error: 0.0283 - mean_squared_error: 0.0155 - val_loss: 3.6169 - val_acc: 0.3897 - val_mean_absolute_error: 0.0737 - val_mean_squared_error: 0.0582\n",
            "Epoch 12/20\n",
            "1088/1088 [==============================] - 83s 77ms/sample - loss: 0.4434 - acc: 0.8585 - mean_absolute_error: 0.0239 - mean_squared_error: 0.0125 - val_loss: 3.8351 - val_acc: 0.4118 - val_mean_absolute_error: 0.0729 - val_mean_squared_error: 0.0572\n",
            "Epoch 13/20\n",
            "1088/1088 [==============================] - 83s 77ms/sample - loss: 0.2931 - acc: 0.9026 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0092 - val_loss: 3.6445 - val_acc: 0.4228 - val_mean_absolute_error: 0.0698 - val_mean_squared_error: 0.0551\n",
            "Epoch 14/20\n",
            "1088/1088 [==============================] - 82s 76ms/sample - loss: 0.3428 - acc: 0.8796 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0104 - val_loss: 4.3054 - val_acc: 0.3971 - val_mean_absolute_error: 0.0716 - val_mean_squared_error: 0.0587\n",
            "Epoch 15/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.3106 - acc: 0.8980 - mean_absolute_error: 0.0165 - mean_squared_error: 0.0091 - val_loss: 3.2081 - val_acc: 0.4559 - val_mean_absolute_error: 0.0667 - val_mean_squared_error: 0.0506\n",
            "Epoch 16/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.3139 - acc: 0.8961 - mean_absolute_error: 0.0164 - mean_squared_error: 0.0091 - val_loss: 4.0585 - val_acc: 0.3971 - val_mean_absolute_error: 0.0741 - val_mean_squared_error: 0.0582\n",
            "Epoch 17/20\n",
            "1088/1088 [==============================] - 82s 76ms/sample - loss: 0.2549 - acc: 0.9145 - mean_absolute_error: 0.0144 - mean_squared_error: 0.0075 - val_loss: 4.4601 - val_acc: 0.3676 - val_mean_absolute_error: 0.0741 - val_mean_squared_error: 0.0606\n",
            "Epoch 18/20\n",
            "1088/1088 [==============================] - 82s 76ms/sample - loss: 0.2341 - acc: 0.9256 - mean_absolute_error: 0.0123 - mean_squared_error: 0.0065 - val_loss: 4.5410 - val_acc: 0.3934 - val_mean_absolute_error: 0.0713 - val_mean_squared_error: 0.0613\n",
            "Epoch 19/20\n",
            "1088/1088 [==============================] - 84s 77ms/sample - loss: 0.2692 - acc: 0.9072 - mean_absolute_error: 0.0147 - mean_squared_error: 0.0078 - val_loss: 3.5338 - val_acc: 0.4301 - val_mean_absolute_error: 0.0672 - val_mean_squared_error: 0.0546\n",
            "Epoch 20/20\n",
            "1088/1088 [==============================] - 83s 76ms/sample - loss: 0.2499 - acc: 0.9127 - mean_absolute_error: 0.0136 - mean_squared_error: 0.0075 - val_loss: 4.9857 - val_acc: 0.3897 - val_mean_absolute_error: 0.0720 - val_mean_squared_error: 0.0632\n"
          ]
        }
      ],
      "source": [
        "#Overfitted model\n",
        "\n",
        "model=Sequential()\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96,input_shape=(224,224,3),kernel_size=(11,11),strides=(4,4)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#1st maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#2nd convolutional layer\n",
        "model.add(Conv2D(filters=256,kernel_size=(5,5),padding='valid',strides=(1,1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#2st maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#3rd convolutional layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#3rd maxpooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
        " metrics=['accuracy','mae','mse'])\n",
        "\n",
        "# (5) Train\n",
        "score = model.fit(x, y, batch_size=64, epochs=20, verbose=1,\\\n",
        "validation_split=0.2, shuffle=True)\n",
        "\n",
        "\n",
        "json.dump(score.history, open(\"OverfittedModel\", 'w'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}